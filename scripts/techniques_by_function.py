# -*- coding: utf-8 -*-
"""Random_forest_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNDbJWP8feah7oIPyqnuUUfJJ1LL19xt
"""

##

# Le dataset est enregistre sous le nom winequality-red.csv.
# Les images sont enregistrees en .png dans votre repertoire courant.

##

# importations
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import GridSearchCV
import os
import pickle
from sklearn.model_selection import train_test_split

currentdirectory = os.getcwd()

# Dataset
df = pd.read_csv(currentdirectory + '/winequality-red.csv')

# The target variable is 'quality'.
target = 'quality'
Y = df[target]
X = df.drop(target, axis = 1)

# Split the data into train and test data:
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

# Model
model = RandomForestClassifier(max_depth=6, random_state=0, n_estimators=1000)
model.fit(X_train, Y_train)


if isinstance(X_test, np.ndarray):
  X_test = pd.DataFrame(X_test, columns=["col{}".format(k) for k in range(X_test.shape[1])])
if isinstance(X_train, np.ndarray):
  X_train = pd.DataFrame(X_train, columns=["col{}".format(k) for k in range(X_train.shape[1])])
if isinstance(Y_test, np.ndarray):
  Y_test = pd.DataFrame(Y_test, columns=["col{}".format(k) for k in range(Y_test.shape[1])])
if isinstance(Y_train, np.ndarray):
  Y_train = pd.DataFrame(Y_train, columns=["col{}".format(k) for k in range(Y_train.shape[1])])

## Uncomment if you want to save your model and data

# np.save('X_train.npy', X_train)
# np.save('X_test.npy', X_test)
# np.save('Y_train', Y_train)
# np.save('Y_test', Y_test)

# pickle.dump(model, open("Random_forest_model.pkl", 'wb'))

## PDP

from pdpbox import pdp

def PDP (model, dataset, X_train, X_test, Y_train, Y_test) :

  numero = 0

  for column in X_train.columns :

    pdp_goals = pdp.pdp_isolate(model=model, dataset = X_train, model_features=X_train.columns.tolist(), feature=column)

    # plot it
    fig = pdp.pdp_plot(pdp_goals, column)
    fig[0].set_size_inches(5, 5)
    plt.tight_layout()
    plt.savefig(currentdirectory + "/pdp{}.png".format(numero))
    numero += 1
    plt.show()

PDP (model, df, X_train, X_test, Y_train, Y_test)

## ICE


def ICE (model, dataset, X_train, X_test, Y_train, Y_test) :

  numero = 0

  for column in X_train.columns :

    pdp_goals = pdp.pdp_isolate(model=model, dataset = X_train, model_features=X_train.columns.tolist(), feature=column)

    # plot it
    fig = pdp.pdp_plot(pdp_goals, column, n_cluster_centers=20, plot_lines=True)
    fig[0].set_size_inches(5, 5)
    plt.tight_layout()
    plt.savefig(currentdirectory + "/ice{}.png".format(numero))
    numero += 1
    plt.show()

ICE (model, df, X_train, X_test, Y_train, Y_test)

## Permutation importance

import eli5
from eli5.sklearn import PermutationImportance


def Permutation_Importance (model, dataset, X_train, X_test, Y_train, Y_test) :

  perm = PermutationImportance(model, random_state=1).fit(X_test, Y_test)
  eli5.show_weights(perm, feature_names = X_test.columns.tolist())

Permutation_Importance (model, df, X_train, X_test, Y_train, Y_test)


## SHAP

import shap

def SHAP (model, dataset, X_train, X_test, Y_train, Y_test) :

  numero = 0
  shap_values = shap.TreeExplainer(model).shap_values(X_train, check_additivity=False)
  shap.summary_plot(shap_values, X_train, plot_type="bar", plot_size = (5, 5))
  plt.tight_layout()
  plt.savefig(currentdirectory + "/shap{}.png".format(numero))
  numero += 1

  for column in X_train.columns :

    if isinstance(shap_values, list) :

      for classe in range(len(shap_values)) :

        fig = plt.figure(figsize = (5, 5))
        ax = fig.add_subplot(111)
        shap.dependence_plot(column, shap_values[classe], X_train, ax = ax)
        plt.tight_layout()
        plt.savefig(currentdirectory + "/shap{}.png".format(numero))
        numero += 1

    else :

      fig = plt.figure(figsize = (5, 5))
      ax = fig.add_subplot(111)
      shap.dependence_plot(column, shap_values, X_train, ax = ax)
      plt.tight_layout()
      plt.savefig(currentdirectory + "/shap{}.png".format(numero))
      numero += 1


SHAP (model, df, X_train, X_test, Y_train, Y_test)

## Feature importance

def Feature_importance (model, dataset, X_train, X_test, Y_train, Y_test) :

	importance = model.feature_importances_

	# plot feature importance
	fig = plt.figure(figsize = (5, 5))
	ax = fig.add_subplot(111)
	plt.bar([x for x in range(len(importance))], importance)
	plt.xlabel("Features")
	plt.tight_layout()
	plt.savefig(currentdirectory + "/feature_importance.png")
	plt.show()

Feature_importance (model, df, X_train, X_test, Y_train, Y_test)

## LIME

import lime
import lime.lime_tabular

def LIME (model, dataset, X_train, X_test, Y_train, Y_test) :

  explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode='regression',training_labels=Y_train, feature_names=df.columns)
  explanation = explainer.explain_instance(X_test.values[0], model.predict)
  explanation.show_in_notebook()